/*! \mainpage Glast Tracker Measurement of Zenith Angle and Azimuth Angle distributions of cosmic rays
 * The goal of this project is to measure the zenith and azimuth angle distribution of cosmic rays using the Fermi-Glast Tracker. \n 
 * ![“Foto dell’apparato, la direzione “y” è uscente, la direzione x è invece verso la sinistra.”](TrackerFoto.jpeg){html: width = 10%, latex: width = 10 }
 * ![“Disposizione dei vari layer del tracker”](TrackerSchema.png){html: width = 10%, latex: width = 10 }\n 
 * In these photos the tracker is portrayed and a faithful succession(not to scale!) along the z axis of the layers is also portrayed. The y-direction is exiting from the sheet, the x-direction is on the left.
 *
  * \section dependencies Dependencies
 *  TESTED ON:
 * - Fedora 40
 * - Linux Mint 
 * - NOT TESTED BUT MIGHT WORK ON: MacOS 
 * 
 * DEPENDENCIES: \n 
 *The code is made up of c++ libraries and macros called by an interface in python, root Cern libraries are used.\n 
  * ROOT CERN VERSION >= 6.30.06 \n 
 * The used root version MUST BE >= 6.30.06 , this is due to a critical code breaking bug in the earlier versions: the Fit method from the TH1F class inherited from the TF1 class has a memory leak that leads to the slowing down of the code and ultimately leads to a crash of the PC. \n 
 *
 * - to install root on Fedora 40 run in terminal: \n
 * $ sudo dnf install root root-minuit2 root-cli python36-root root-core root-cling \n 
 *Package : https://packages.fedoraproject.org/pkgs/root/ \n 
 * - to install root on debian based os (Ubuntu, Linux) check site: 
 * https://root.cern/install/\n 
 * 
 * PYTHON DEPENDENCIES: pyroot , loguru, multiprocess (NOT MULTIPROCESSING!!!) \n 
 * - on Fedora 40 run in terminal : \n 
 * $ pip install loguru multiprocess 
 * - on Linux run in terminal : \n 
 * $ pip install loguru pyroot multiprocess
 *
 *
 *
 *
 * \section setup_sec Setup
 *
 * 
 * To setup the package, firstly run in the Core folder:\n 
 * $ cmake ..\n 
 * Then run \n 
 * $ make -jN\n 
 * where N is the number of cores your PC has, one can alternatively just run on one core and do:\n 
 * $ make\n 
 *
 * \section intro_sec Introduction and brief resume of the project
 *The aim of this project is to measure the zenith and azimuth angular distribution of Cosmic Rays using 10 layers of the Fermi-Glast Tracker, each layer contains 1536 strips that are ideally parallel to each other. \n
 * The layers are subdivided into two main groups XN and YN each composed of 5 layers, those that have strips parallel to a “x” direction and those who have strips parallel to a “y” direction that is perpendicular to the “x” direction.
 * The used tracker returns info on the projection of the cosmic rays, an acquisition is triggered if for each of the vision XZ and YZ the following condition is satisfied:
 * - >= 3 CONSECUTIVE hits
 * It may happen that some of the events do not satisfy this trigger condition due to the premature fall below threshold of the signal generated by the strips as there is a minimum delay of o(100 ns) from the request to start an acquisition and the data taking. In fact, those events amount to about 8%.
 * \subsection first_ab First Data Abstraction Phase: Clustering
 * A first abstraction of the data is made from hits to cluster by calling the macro CreateTree.cpp. A cluster is defined as a series of consecutive strips. \n 
 * A tree is filled with instances of the class Evento, this object contains info on the hits, their respective layers; info on the position, dimension, initial strip of the clusters and a vector of flags. \n 
 * The most important flag to account of is the first: this flag is set to False if there are in the event clusters adjacent to dead/muted/inefficient strips (strips that have not been activated) . The events that do not satisfy the flag amounts to about 21 %, those are kept but not used in further abstraction of the data.
 *
 *\subsection second_ab Second Data Abstraction Phase: Tracking
 * The second abstraction of the phase corresponds to tracking, a track is defined as >= 3 up to 5 clusters (max 1 per layer) for which a linear fit returns a chisquare that is below a 0.95 p-value cut made from the nominal distribution of the chisquare at N dof. The actual studied object are the projections  of the tracks rather than the tracks itself. \n 
 * Potential projections of the tracks are identified using a tracking algorithm inspired to the Retina tracking algorithm, which checks for each layer of each vision clusters that are close by 0.5 cm from a straight line with value (m, q); the closest clusters is chosen. 
 * This is run on each cell a phase space of (m,q). The obtained info on the clusters and on the linear fit is inserted for each projection on instances of the object Track, all of those instances are then inserted into instances of the object EventoTrack and a root tree object is filled and inserted into a root file. The Object Track contains overloaded operator for > and ==, a Track is “greater” than another if it contains more clusters, a Track is “equal” to another if it contains more than 1 cluster in common. Those operators have been inserted to account for the fact that the tracking algorithm after identifying a track does not remove the points from the events, and so between adjacent phase space cells there may be identified projections that are actually the same one.
 * It has been decided to not remove the points as that may actually introduce dangerous biases. \n 
 * After performing identification of potential projections, systematic error due to relative shifts in X, Y , Z and rotations between the Layers are corrected, in this project the macros to calculate this corrections are not include. However, here we report the way those corrections have been calculated.\n 
 * Firstly, the population of events selected is the one that satisfy the second Flag of the object Evento, which is a maximum of 1 cluster per layer. We assume that those contains projections of the tracks. \n 
 * After chosing this populations, the shifts are calculated in the following order:
 * - Shifts on X, Y: those are calculated by keeping the layers X0, X3 and Y1 , Y4 fixed, the events chosen from the original population are those that have vertical projection on both vision in order to decorrelate the calculation from the shifts along the Z direction. In those events if there are clusters in the fixed layers a straight line that passes between those is drawn, the distance in along the X or Y directions between the clusters are calculated from this line for each other layer: after filling an histogram with the the distances along the direction X or Y for each layer the mean is used as shift correction.
 * - Shifts on Z: after performing the shift correction on the X, Y directions, all the events from the population with the second Flag activated are taken. Like in the calculation for the shifts in X, Y the layers X0, X3, Y1 and Y4 are fixed in position, if in an event clusters are in those fixed layers then a straight line is drawn and for each layer XN and YN the distances along the direction Z between the clusters and the drawn line are then inserted in a histogram for each layer,  the means are taken as shifts on the Z directions.
 * -Relative rotations on Planes: This correction is possible only in case there is 1 projection per vision as in this case one can affirm that the projections belong to the same 3D track. This correction is iterative. After performing the shift corrections along the X, Y, Z directions, for each layer fits are made by excluding the cluster in the layer, then residuals along the distance X or Y depending on the layer are correlated to correspondent position on the other vision which can be calculated since there are no ambiguities in the 3D reconstruction. A linear fit is made and then correction are fed on the position of the clusters at each iteration, ultimately the corrections is stopped when the angular coefficient of the fitted line in the residuals vs hit position along the strip plot is close enough to 0. \n
*- After all the corrections, errors on each layers are set to the standard deviation of the residuals after performing fits excluding clusters from the layer , and a cut is made at a nominal p-value of 0.95. \n
- After this cut, the zenith and azimuth angle can be calculated and the tracking phase abstraction is concluded.
 * \subsection first_sec_first_sub_sec Typical workflow
 * The package is structured in the following way: An interface is written in python language and firstly load using a rootlogon.C file the shared utils libraries written in C++.
 * The Interface.py can be run with multiple options, the usual workflow is the following:
 *- $ python -i Interface.py -i infile.lif -o outfileCluster -Clustering \n 
Performs the first abstraction of data from hits to clusters\n 
 *- $ python -i Interface.py -i infile.lif -o outfileMC -MCDist \n 
Generates MC expected measured distributions for the zenith and azimuth angles simulating dead strips and mean efficiency of the layers. The dimension of the clusters, smearing of residuals are not taken into account and other refinements are not taken into account.\n 
* This option prints out the simulated Hits distributions for all layers, the generated pdf’s and the expected measured zenith and azimuth angle distributions. The MC assumes a distribution sin(theta)cos^3(theta) for the zenith angle theta and a uniform distribution between -180 and 180 deg for the azimuth angle.\n  
 *- $ python -i Interface.py -i outfileCluster.root -o outfileTracks -Tracking\n 
Performs the second abstraction of data from clusters to tracks, prints out the chisquare distribution with dof = 3 before and after correcting the systematics.
*As said, at the end of the script a file with a cut in chisquare is made. \n 
*A note: this part of the code is the most intensive one: time tests have been *run on 2 PCs with a 100k events sample data: \n 
*-) i7 11th gen 8-core , clockspeed 2.50 GHz time elapsed about 400s \n 
*-) i3 6th gen 4-core, clockspeed 2.30 GHz, time elapsed about 900s \n 
*One may use those to calculate an “expected” time run.
 *- $ python -i Interface.py -i outfileTracks.root -Results\n 
 *Display the results and returns the p-value from a kolmogorov tests between \n  measured distributions and Montecarlo simulated ones. \n 
 *Other options are 
 * - $ python -i Interface.py -TestReconstruction \n 
 * The -i option after python is put to keep alive the processes containing the produced TH1F and TCanvas instances.
 * Tests the reconstruction made by the tracking algorithm by feeding it a generated distribution of events that have one projection per vision XZ and YZ. The number of identified projection is reported and compared with the expected one, the efficiency of the algorithm is calculated dividing the number of events that have 1 reconstructed track per layer by the total number of generated events. After this first steps a kolmogorov tests is taken and the zenith and azimuth angle reconstructed distributions are compared to the generated ones, a p-value is returned for both comparisons. \n 
 * - $ python -i Interface.py -i outfileTracks.root -nx N -ny M \n 
 * Displays an event with N projections on the XZ vision and M projections on the YZ vision.
 *
 *
 *
 *
 *\section sec_otherstuff Other Improvements
 * Multitracking analysis has not been included in this project, however one may easily do multitracking 2D analysis just by studying the distribution of angles between tracks, comparison can be made between the distributions of angles between projections in the same events and angles between projections from different events (to take away the correlation). \n 
* One could also study the vertex distributions in the projection planes. \n 
 *Also one may introduce heavy materials such as lead to study muon decays, however this is much more ambitious than the previous ideas.\n 
 *
 *
 *
 */
